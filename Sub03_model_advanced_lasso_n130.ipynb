{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a7fc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmtk49\\anaconda3\\envs\\py38_fastai\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\kmtk49\\anaconda3\\envs\\py38_fastai\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\Users\\kmtk49\\anaconda3\\envs\\py38_fastai\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "import numerapi, os, datetime,time,gc\n",
    "from numerapi import NumerAPI\n",
    "from utils import save_model, load_model, neutralize, get_biggest_change_features, validation_metrics, download_data, \\\n",
    "    load_model_config, save_model_config, get_time_series_cross_val_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "187ece38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Get your API keys and model_id from https://numer.ai/submit\n",
    "public_id = \"TIFYUYGPJCFQZT4SBLNYIOZWQWIMC2EQ\"\n",
    "secret_key = \"QGLW6MHS7QIDPEELKVJ6DKUGBWHQMN7O3A2BCIV7CU4QKGTNVBZ7F5RRFF75M4DB\"\n",
    "model_1 = \"45b2b9e3-ed1b-4d82-bb2c-84f828b403fe\"\n",
    "model_2 = 'f624d92f-3965-4242-b069-dda40993fffa'\n",
    "model_3 ='b58e9477-feb6-4cd3-ae53-290326a41ecd'\n",
    "napi = numerapi.NumerAPI(public_id=public_id, secret_key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98a77a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "EXAMPLE_PREDS_COL = \"example_preds\"\n",
    "TARGET_COL = \"target\"\n",
    "ERA_COL = \"era\"\n",
    "PATH = \"E:/Anaconda3E/Numerai/massive_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7332500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering model selection loop.  This may take awhile.\n",
      "downloading training_data\n",
      "- Downloading numerai_training_data.parquet\\ Downloading numerai_training_data.parquet| Downloading numerai_training_data.parquet/ Downloading numerai_training_data.parquet- Downloading numerai_training_data.parquet\\ Downloading numerai_training_data.parquet| Downloading numerai_training_data.parquet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 16:19:23,546 INFO numerapi.utils: target file already exists\n",
      "2021-09-20 16:19:23,547 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v Downloading numerai_training_data.parquet\n",
      "reading training data from local file\n",
      "entering time series cross validation loop\n",
      "doing split 1 out of 3\n",
      "getting feature correlations over time and identifying riskiest features\n",
      "entering model training loop for split 1\n",
      "model: model_target\n",
      "New features are available! Might want to retrain model model_target_split1cv3downsample20.\n",
      "predicting model_target\n",
      "doing neutralization to riskiest features\n",
      "model: model_target_nomi_60\n",
      "New features are available! Might want to retrain model model_target_nomi_60_split1cv3downsample20.\n",
      "predicting model_target_nomi_60\n",
      "doing neutralization to riskiest features\n",
      "model: model_target_jerome_20\n",
      "New features are available! Might want to retrain model model_target_jerome_20_split1cv3downsample20.\n",
      "predicting model_target_jerome_20\n",
      "doing neutralization to riskiest features\n",
      "creating ensembles\n",
      "doing split 2 out of 3\n",
      "getting feature correlations over time and identifying riskiest features\n",
      "entering model training loop for split 2\n",
      "model: model_target\n",
      "New features are available! Might want to retrain model model_target_split2cv3downsample20.\n",
      "predicting model_target\n",
      "doing neutralization to riskiest features\n",
      "model: model_target_nomi_60\n",
      "New features are available! Might want to retrain model model_target_nomi_60_split2cv3downsample20.\n",
      "predicting model_target_nomi_60\n",
      "doing neutralization to riskiest features\n",
      "model: model_target_jerome_20\n",
      "New features are available! Might want to retrain model model_target_jerome_20_split2cv3downsample20.\n",
      "predicting model_target_jerome_20\n",
      "doing neutralization to riskiest features\n",
      "creating ensembles\n",
      "doing split 3 out of 3\n",
      "getting feature correlations over time and identifying riskiest features\n",
      "entering model training loop for split 3\n",
      "model: model_target\n",
      "New features are available! Might want to retrain model model_target_split3cv3downsample20.\n",
      "predicting model_target\n",
      "doing neutralization to riskiest features\n",
      "model: model_target_nomi_60\n",
      "New features are available! Might want to retrain model model_target_nomi_60_split3cv3downsample20.\n",
      "predicting model_target_nomi_60\n",
      "doing neutralization to riskiest features\n",
      "model: model_target_jerome_20\n",
      "New features are available! Might want to retrain model model_target_jerome_20_split3cv3downsample20.\n",
      "predicting model_target_jerome_20\n",
      "doing neutralization to riskiest features\n",
      "creating ensembles\n",
      "gathering validation metrics for out of sample training results\n",
      "|                                                  |      mean |   sharpe |\n",
      "|:-------------------------------------------------|----------:|---------:|\n",
      "| ensemble_neutral_riskiest_50                     | 0.0412893 |  1.80193 |\n",
      "| ensemble_all                                     | 0.0443156 |  1.78322 |\n",
      "| ensemble_not_neutral                             | 0.0459149 |  1.72076 |\n",
      "| preds_model_target_jerome_20_neutral_riskiest_50 | 0.036678  |  1.66869 |\n",
      "| preds_model_target_neutral_riskiest_50           | 0.0349824 |  1.66254 |\n",
      "| preds_model_target                               | 0.0398028 |  1.64682 |\n",
      "| preds_model_target_nomi_60                       | 0.0418597 |  1.60583 |\n",
      "| preds_model_target_nomi_60_neutral_riskiest_50   | 0.0372949 |  1.60419 |\n",
      "| preds_model_target_jerome_20                     | 0.040659  |  1.58296 |\n",
      "selecting model ensemble_neutral_riskiest_50 as our highest sharpe model in validation\n",
      "entering full training section\n",
      "getting feature correlations with target and identifying riskiest features\n",
      "saving model config for advanced_example_model\n",
      "\r"
     ]
    }
   ],
   "source": [
    "# params we'll use to train all of our models.\n",
    "# Ideal params would be more like 20000, 0.001, 6, 2**6, 0.1, but this is slow enough as it is\n",
    "model_params = {\"n_estimators\": 2000,\n",
    "                \"learning_rate\": 0.01,\n",
    "                \"max_depth\": 5,\n",
    "                \"num_leaves\": 2 ** 5,\n",
    "                \"colsample_bytree\": 0.1}\n",
    "\n",
    "# the amount of downsampling we'll use to speed up cross validation and full train.\n",
    "# a value of 1 means no downsampling\n",
    "# a value of 10 means use every 10th row\n",
    "downsample_cross_val = 20\n",
    "downsample_full_train = 1\n",
    "\n",
    "# if model_selection_loop=True get OOS performance for training_data\n",
    "# and use that to select best model\n",
    "# if model_selection_loop=False, just predict on tournament data using existing models and model config\n",
    "model_selection_loop = True\n",
    "model_config_name = \"advanced_example_model\"\n",
    "\n",
    "napi = NumerAPI()\n",
    "\n",
    "current_round = napi.get_current_round(tournament=8)  # tournament 8 is the primary Numerai Tournament\n",
    "\n",
    "print(\"Entering model selection loop.  This may take awhile.\")\n",
    "if model_selection_loop:\n",
    "    model_config = {}\n",
    "    print('downloading training_data')\n",
    "    download_data(napi, 'numerai_training_data.parquet', 'numerai_training_data.parquet')\n",
    "\n",
    "    print(\"reading training data from local file\")\n",
    "    training_data = pd.read_parquet('numerai_training_data.parquet')\n",
    "\n",
    "    # keep track of some prediction columns\n",
    "    ensemble_cols = set()\n",
    "    pred_cols = set()\n",
    "\n",
    "    # pick some targets to use\n",
    "    possible_targets = [c for c in training_data.columns if c.startswith(\"target_\")]\n",
    "    # randomly pick a handful of targets\n",
    "    # this can be vastly improved\n",
    "    targets = [\"target\", \"target_nomi_60\", \"target_jerome_20\"]\n",
    "\n",
    "    # all the possible features to train on\n",
    "    #feature_cols = [c for c in training_data if c.startswith(\"feature_\")]\n",
    "    # https://forum.numer.ai/t/feature-selection-with-borutashap/4145\n",
    "    feature_cols = ['feature_hypertrophied_embryologic_forfeiter',\n",
    " 'feature_astronomic_gripple_culverin',\n",
    " 'feature_phantasmal_stomachal_peperoni',\n",
    " 'feature_subapostolic_dungy_fermion',\n",
    " 'feature_lipogrammatic_blowsier_seismometry',\n",
    " 'feature_unawakening_escapism_totemist',\n",
    " 'feature_gabby_glimmery_azalea',\n",
    " 'feature_mesoblastic_anatolian_deodand',\n",
    " 'feature_unministerial_albinotic_salish',\n",
    " 'feature_naval_edified_decarbonize',\n",
    " 'feature_aliped_subclavicular_nasopharynx',\n",
    " 'feature_autarkic_constabulary_dukedom',\n",
    " 'feature_calefactive_anapaestic_jerome',\n",
    " 'feature_unemotional_quietistic_chirper',\n",
    " 'feature_submediant_serbonian_rangefinder',\n",
    " 'feature_unblamed_mammoth_commie',\n",
    " 'feature_continental_archival_katmandu',\n",
    " 'feature_juvenalian_paunchy_uniformitarianism',\n",
    " 'feature_radiculose_perfectionistic_swing',\n",
    " 'feature_clarified_mimic_slash',\n",
    " 'feature_attuned_southward_heckle',\n",
    " 'feature_hexametric_ventricose_limnology',\n",
    " 'feature_myographic_gawkier_timbale',\n",
    " 'feature_unsolvable_majuscular_caique',\n",
    " 'feature_mucky_loanable_gastrostomy',\n",
    " 'feature_dichasial_hammier_spawner',\n",
    " 'feature_halting_disguisable_syllabicity',\n",
    " 'feature_yankee_nonary_okavango',\n",
    " 'feature_difficile_oxalic_spheroid',\n",
    " 'feature_subglobular_unsalable_patzer',\n",
    " 'feature_flintier_enslaved_borsch',\n",
    " 'feature_contralto_unblushing_beowulf',\n",
    " 'feature_unwarrantable_egotistic_guayule',\n",
    " 'feature_ovular_powered_neckar',\n",
    " 'feature_leaky_maroon_pyrometry',\n",
    " 'feature_newish_eosinophilic_sciaenoid',\n",
    " 'feature_mephitic_televisionary_breeder',\n",
    " 'feature_deceased_peppiest_hitlerism',\n",
    " 'feature_daytime_suppurative_oximeter',\n",
    " 'feature_unnetted_bay_premillennialist',\n",
    " 'feature_interoceptive_fifteenth_trey',\n",
    " 'feature_exterminated_grumbling_lawing',\n",
    " 'feature_accusatory_disinfectant_deportment',\n",
    " 'feature_salian_suggested_ephemeron',\n",
    " 'feature_passerine_ultraist_neon',\n",
    " 'feature_terrific_epigamic_affectivity',\n",
    " 'feature_interrogatory_isohyetal_atacamite',\n",
    " 'feature_muskiest_transcendentalist_pantheism',\n",
    " 'feature_quaggy_constabulary_mismanagement',\n",
    " 'feature_livelong_sounded_squirm',\n",
    " 'feature_vaunting_offshore_bogey',\n",
    " 'feature_unstacked_trackable_blizzard',\n",
    " 'feature_illiterate_stomachal_terpene',\n",
    " 'feature_pinioned_desiccated_renegade',\n",
    " 'feature_zarathustrian_albigensian_itch',\n",
    " 'feature_bleeding_arabesque_pneuma',\n",
    " 'feature_ctenoid_moaning_fontainebleau',\n",
    " 'feature_screwy_asianic_balanchine',\n",
    " 'feature_decent_solo_stickup',\n",
    " 'feature_unsealed_suffixal_babar',\n",
    " 'feature_unspotted_practiced_gland',\n",
    " 'feature_semibold_disparaging_turnip',\n",
    " 'feature_instrumentalist_extrovert_cassini',\n",
    " 'feature_evaporative_largo_aster',\n",
    " 'feature_systematic_nappiest_bruiser',\n",
    " 'feature_hydrologic_cymric_nyctophobia',\n",
    " 'feature_plutonian_unsegregated_meringue',\n",
    " 'feature_mouldiest_clostridial_paper',\n",
    " 'feature_budgetary_surrounded_formicary',\n",
    " 'feature_adynamic_glossier_ghana',\n",
    " 'feature_lost_quirky_botel',\n",
    " 'feature_indirect_concrete_canaille',\n",
    " 'feature_unaired_operose_lactoprotein',\n",
    " 'feature_unforbidden_flaming_settlement',\n",
    " 'feature_draconic_contractible_romper',\n",
    " 'feature_fleshly_bedimmed_enfacement',\n",
    " 'feature_covalent_unreformed_frogbit',\n",
    " 'feature_chaldean_vixenly_propylite',\n",
    " 'feature_tercentenary_triliteral_formosa',\n",
    " 'feature_cromwellian_imbecilic_algonkian',\n",
    " 'feature_unsurveyed_boyish_aleph',\n",
    " 'feature_fustiest_voiced_janet',\n",
    " 'feature_assertive_worsened_scarper',\n",
    " 'feature_mobile_savoyard_maidenhead',\n",
    " 'feature_gainly_tritheism_syndactyl',\n",
    " 'feature_heliocentric_stealthy_waning',\n",
    " 'feature_communicatory_unrecommended_velure',\n",
    " 'feature_expressed_abhominable_pruning',\n",
    " 'feature_thousandfold_cairned_commutator',\n",
    " 'feature_sudsy_polymeric_posteriority',\n",
    " 'feature_invincible_unroofed_tetrachord',\n",
    " 'feature_agile_unrespited_gaucho',\n",
    " 'feature_refreshed_untombed_skinhead',\n",
    " 'feature_mailed_harried_grant',\n",
    " 'feature_multilinear_sharpened_mouse',\n",
    " 'feature_hotshot_undefied_aten',\n",
    " 'feature_branched_dilatory_sunbelt',\n",
    " 'feature_virtual_standard_instigator',\n",
    " 'feature_undersexed_agone_metaplasm',\n",
    " 'feature_resoluble_shortened_albumin',\n",
    " 'feature_illegible_punishing_lunacy',\n",
    " 'feature_polymorphic_climactic_corslet',\n",
    " 'feature_carnassial_undecided_suspensor',\n",
    " 'feature_sludgiest_telltale_mutant',\n",
    " 'feature_gradely_dippy_gaol',\n",
    " 'feature_spongiest_sunbeamed_connolly',\n",
    " 'feature_seamier_jansenism_inflator',\n",
    " 'feature_eruptive_seasoned_pharmacognosy',\n",
    " 'feature_confiscatory_triennial_pelting',\n",
    " 'feature_grazed_blameful_desiderative',\n",
    " 'feature_motherly_chalkier_woking',\n",
    " 'feature_hypersonic_volcanological_footwear',\n",
    " 'feature_overfraught_chinked_jinx',\n",
    " 'feature_escutcheoned_timocratic_kotwal',\n",
    " 'feature_exacerbating_presentationism_apagoge',\n",
    " 'feature_unsyllabled_skin_underworkman',\n",
    " 'feature_obeisant_vicarial_passibility',\n",
    " 'feature_passable_premed_independent',\n",
    " 'feature_lageniform_theodicean_terylene',\n",
    " 'feature_afloat_brickiest_supernationalism',\n",
    " 'feature_tranquilizing_abashed_glyceria',\n",
    " 'feature_voltairean_dyslogistic_epagoge',\n",
    " 'feature_ternary_plump_floridity',\n",
    " 'feature_unsafe_perspectivist_dairyman',\n",
    " 'feature_neurophysiological_runcinate_yardang',\n",
    " 'feature_nucleophilic_uremic_endogen',\n",
    " 'feature_irradiative_unpolished_sodality',\n",
    " 'feature_hungry_thermoluminescent_shareholding',\n",
    " 'feature_antisubmarine_foregoing_cryosurgery',\n",
    " 'feature_myological_adducent_puebla']\n",
    "    \"\"\" do cross val to get out of sample training preds\"\"\"\n",
    "    cv = 3\n",
    "    train_test_zip = get_time_series_cross_val_splits(training_data, cv=cv, embargo=12)\n",
    "    # get out of sample training preds via embargoed time series cross validation\n",
    "    # optionally downsample training data to speed up this section.\n",
    "    print(\"entering time series cross validation loop\")\n",
    "    for split, train_test_split in enumerate(train_test_zip):\n",
    "        gc.collect()\n",
    "        print(f\"doing split {split+1} out of {cv}\")\n",
    "        train_split, test_split = train_test_split\n",
    "        train_split_index = training_data[ERA_COL].isin(train_split)\n",
    "        test_split_index = training_data[ERA_COL].isin(test_split)\n",
    "        downsampled_train_split_index = train_split_index[train_split_index].index[::downsample_cross_val]\n",
    "\n",
    "        # getting the per era correlation of each feature vs the primary target across the training split\n",
    "        print(\"getting feature correlations over time and identifying riskiest features\")\n",
    "        all_feature_corrs_split = training_data.loc[downsampled_train_split_index, :].groupby(ERA_COL).apply(\n",
    "            lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\n",
    "        # find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\n",
    "        # there are probably more clever ways to do this\n",
    "        riskiest_features_split = get_biggest_change_features(all_feature_corrs_split, 50)\n",
    "\n",
    "        print(f\"entering model training loop for split {split+1}\")\n",
    "        for target in targets:\n",
    "            model_name = f\"model_{target}\"\n",
    "            print(f\"model: {model_name}\")\n",
    "\n",
    "            # train a model on the training split (and save it for future use)\n",
    "            split_model_name = f\"model_{target}_split{split+1}cv{cv}downsample{downsample_cross_val}\"\n",
    "            split_model = load_model(split_model_name)\n",
    "            if not split_model:\n",
    "                print(f\"training model: {model_name}\")\n",
    "                split_model = LGBMRegressor(**model_params)\n",
    "                split_model.fit(training_data.loc[downsampled_train_split_index, feature_cols],\n",
    "                                training_data.loc[downsampled_train_split_index,\n",
    "                                                  [target]])\n",
    "                save_model(split_model, split_model_name)\n",
    "            # now we can predict on the test part of the split\n",
    "            model_expected_features = split_model.booster_.feature_name()\n",
    "            if set(model_expected_features) != set(feature_cols):\n",
    "                print(f\"New features are available! Might want to retrain model {split_model_name}.\")\n",
    "            print(f\"predicting {model_name}\")\n",
    "            training_data.loc[test_split_index, f\"preds_{model_name}\"] = \\\n",
    "                split_model.predict(training_data.loc[test_split_index, model_expected_features])\n",
    "\n",
    "            # do neutralization\n",
    "            print(\"doing neutralization to riskiest features\")\n",
    "            training_data.loc[test_split_index, f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n",
    "                df=training_data.loc[test_split_index, :],\n",
    "                columns=[f\"preds_{model_name}\"],\n",
    "                neutralizers=riskiest_features_split,\n",
    "                proportion=0.8,\n",
    "                normalize=True,\n",
    "                era_col=ERA_COL)[f\"preds_{model_name}\"]\n",
    "\n",
    "            # remember that we made all of these different pred columns\n",
    "            pred_cols.add(f\"preds_{model_name}\")\n",
    "            pred_cols.add(f\"preds_{model_name}_neutral_riskiest_50\")\n",
    "\n",
    "        print(\"creating ensembles\")\n",
    "        # ranking per era for all of our pred cols so we can combine safely on the same scales\n",
    "        training_data[list(pred_cols)] = training_data.groupby(ERA_COL).apply(\n",
    "            lambda d: d[list(pred_cols)].rank(pct=True))\n",
    "        # do ensembles\n",
    "        training_data[\"ensemble_neutral_riskiest_50\"] = sum(\n",
    "            [training_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n",
    "            pct=True)\n",
    "        training_data[\"ensemble_not_neutral\"] = sum(\n",
    "            [training_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n",
    "        training_data[\"ensemble_all\"] = sum([training_data[pred_col] for pred_col in pred_cols]).rank(pct=True)\n",
    "\n",
    "        ensemble_cols.add(\"ensemble_neutral_riskiest_50\")\n",
    "        ensemble_cols.add(\"ensemble_not_neutral\")\n",
    "        ensemble_cols.add(\"ensemble_all\")\n",
    "\n",
    "    \"\"\" Now get some stats and pick our favorite model\"\"\"\n",
    "    print(\"gathering validation metrics for out of sample training results\")\n",
    "    all_model_cols = list(pred_cols) + list(ensemble_cols)\n",
    "    # use example_col preds_model_target as an estimates since no example preds provided for training\n",
    "    # fast_mode=True so that we skip some of the stats that are slower to calculate\n",
    "    training_stats = validation_metrics(training_data, all_model_cols, example_col=\"preds_model_target\",\n",
    "                                        fast_mode=True)\n",
    "    print(training_stats[[\"mean\", \"sharpe\"]].sort_values(by=\"sharpe\", ascending=False).to_markdown())\n",
    "\n",
    "    # pick the model that has the highest correlation sharpe\n",
    "    best_pred_col = training_stats.sort_values(by=\"sharpe\", ascending=False).head(1).index[0]\n",
    "    print(f\"selecting model {best_pred_col} as our highest sharpe model in validation\")\n",
    "\n",
    "    \"\"\" Now do a full train\"\"\"\n",
    "    print(\"entering full training section\")\n",
    "    # getting the per era correlation of each feature vs the target across all of training data\n",
    "    print(\"getting feature correlations with target and identifying riskiest features\")\n",
    "    all_feature_corrs = training_data.groupby(ERA_COL).apply(\n",
    "        lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\n",
    "    # find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\n",
    "    riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\n",
    "\n",
    "    for target in targets:\n",
    "        gc.collect()\n",
    "        model_name = f\"model_{target}_downsample{downsample_full_train}\"\n",
    "        model = load_model(model_name)\n",
    "        if not model:\n",
    "            print(f\"training {model_name}\")\n",
    "            model = LGBMRegressor(**model_params)\n",
    "            # train on all of train, predict on val, predict on tournament\n",
    "            model.fit(training_data.iloc[::downsample_full_train].loc[:, feature_cols],\n",
    "                      training_data.iloc[::downsample_full_train][target])\n",
    "            save_model(model, model_name)\n",
    "        gc.collect()\n",
    "\n",
    "    model_config[\"feature_cols\"] = feature_cols\n",
    "    model_config[\"targets\"] = targets\n",
    "    model_config[\"best_pred_col\"] = best_pred_col\n",
    "    model_config[\"riskiest_features\"] = riskiest_features\n",
    "    print(f\"saving model config for {model_config_name}\")\n",
    "    save_model_config(model_config, model_config_name)\n",
    "else:\n",
    "    # load model config from previous model selection loop\n",
    "    print(f\"loading model config for {model_config_name}\")\n",
    "    model_config = load_model_config(model_config_name)\n",
    "    feature_cols = model_config[\"feature_cols\"]\n",
    "    targets = model_config[\"targets\"]\n",
    "    best_pred_col = model_config[\"best_pred_col\"]\n",
    "    riskiest_features = model_config[\"riskiest_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b9b863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading tournament_data\n",
      "- Downloading numerai_tournament_data_282.parquet\\ Downloading numerai_tournament_data_282.parquet| Downloading numerai_tournament_data_282.parquet/ Downloading numerai_tournament_data_282.parquet- Downloading numerai_tournament_data_282.parquet\\ Downloading numerai_tournament_data_282.parquet| Downloading numerai_tournament_data_282.parquet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 16:30:40,380 INFO numerapi.utils: target file already exists\n",
      "2021-09-20 16:30:40,381 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v Downloading numerai_tournament_data_282.parquet\n",
      "downloading validation_data\n",
      "- Downloading numerai_validation_data.parquet\\ Downloading numerai_validation_data.parquet| Downloading numerai_validation_data.parquet/ Downloading numerai_validation_data.parquet- Downloading numerai_validation_data.parquet\\ Downloading numerai_validation_data.parquet| Downloading numerai_validation_data.parquet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 16:30:41,150 INFO numerapi.utils: target file already exists\n",
      "2021-09-20 16:30:41,151 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v Downloading numerai_validation_data.parquet\n",
      "downloading example_predictions\n",
      "- Downloading example_predictions_282.parquet\\ Downloading example_predictions_282.parquet| Downloading example_predictions_282.parquet/ Downloading example_predictions_282.parquet- Downloading example_predictions_282.parquet\\ Downloading example_predictions_282.parquet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 16:30:41,927 INFO numerapi.utils: target file already exists\n",
      "2021-09-20 16:30:41,928 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v Downloading example_predictions_282.parquet\n",
      "downloading example_validation_predictions\n",
      "- Downloading example_validation_predictions.parquet\\ Downloading example_validation_predictions.parquet| Downloading example_validation_predictions.parquet/ Downloading example_validation_predictions.parquet- Downloading example_validation_predictions.parquet\\ Downloading example_validation_predictions.parquet| Downloading example_validation_predictions.parquet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 16:30:42,708 INFO numerapi.utils: target file already exists\n",
      "2021-09-20 16:30:42,709 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v Downloading example_validation_predictions.parquet\n",
      "reading tournament_data\n",
      "reading validation_data\n",
      "reading example_predictions\n",
      "reading example_validaton_predictions\n",
      "checking for nans in the tournament data\n",
      "No nans in the features this week!\n",
      "loading model_target_downsample1\n",
      "New features are available! Might want to retrain model model_target_downsample1.\n",
      "predicting tournament and validation for model_target_downsample1\n",
      "neutralizing to riskiest_50 for validation and tournament\n",
      "loading model_target_nomi_60_downsample1\n",
      "New features are available! Might want to retrain model model_target_nomi_60_downsample1.\n",
      "predicting tournament and validation for model_target_nomi_60_downsample1\n",
      "neutralizing to riskiest_50 for validation and tournament\n",
      "loading model_target_jerome_20_downsample1\n",
      "New features are available! Might want to retrain model model_target_jerome_20_downsample1.\n",
      "predicting tournament and validation for model_target_jerome_20_downsample1\n",
      "neutralizing to riskiest_50 for validation and tournament\n",
      "creating ensembles for tournament and validation\n",
      "getting final validation stats\n",
      "|                              |      mean |       std |   sharpe |   max_drawdown |    apy |   max_feature_exposure |   feature_neutral_mean |   tb200_mean |   tb200_std |   tb200_sharpe |   mmc_mean |   corr_plus_mmc_sharpe |   corr_with_example_preds |\n",
      "|:-----------------------------|----------:|----------:|---------:|---------------:|-------:|-----------------------:|-----------------------:|-------------:|------------:|---------------:|-----------:|-----------------------:|--------------------------:|\n",
      "| ensemble_neutral_riskiest_50 | 0.0243663 | 0.0258497 | 0.942611 |     -0.0987974 | 220.29 |               0.380546 |              0.0190404 |    0.0527474 |   0.0571601 |       0.942611 | 0.00390799 |               0.865031 |                  0.757779 |\n",
      "\r"
     ]
    }
   ],
   "source": [
    "\"\"\" Things that we always do even if we've already trained \"\"\"\n",
    "gc.collect()\n",
    "print(\"downloading tournament_data\")\n",
    "download_data(napi, 'numerai_tournament_data.parquet', f'numerai_tournament_data_{current_round}.parquet')\n",
    "print(\"downloading validation_data\")\n",
    "download_data(napi, 'numerai_validation_data.parquet', 'numerai_validation_data.parquet')\n",
    "print(\"downloading example_predictions\")\n",
    "download_data(napi, 'example_predictions.parquet', f'example_predictions_{current_round}.parquet')\n",
    "print(\"downloading example_validation_predictions\")\n",
    "download_data(napi, 'example_validation_predictions.parquet', f'example_validation_predictions.parquet')\n",
    "\n",
    "print(\"reading tournament_data\")\n",
    "tournament_data = pd.read_parquet(f'numerai_tournament_data_{current_round}.parquet')\n",
    "print(\"reading validation_data\")\n",
    "validation_data = pd.read_parquet('numerai_validation_data.parquet')\n",
    "print(\"reading example_predictions\")\n",
    "example_preds = pd.read_parquet(f'example_predictions_{current_round}.parquet')\n",
    "print(\"reading example_validaton_predictions\")\n",
    "validation_example_preds = pd.read_parquet('example_validation_predictions.parquet')\n",
    "# set the example predictions\n",
    "validation_data[EXAMPLE_PREDS_COL] = validation_example_preds[\"prediction\"]\n",
    "\n",
    "# check for nans and fill nans\n",
    "print(\"checking for nans in the tournament data\")\n",
    "if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\n",
    "    cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\n",
    "    total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\n",
    "    print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\n",
    "    print(f\"out of {total_rows} total rows\")\n",
    "    print(f\"filling nans with 0.5\")\n",
    "    tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\n",
    "else:\n",
    "    print(\"No nans in the features this week!\")\n",
    "\n",
    "\n",
    "pred_cols = set()\n",
    "ensemble_cols = set()\n",
    "for target in targets:\n",
    "    gc.collect()\n",
    "    model_name = f\"model_{target}_downsample{downsample_full_train}\"\n",
    "    print(f\"loading {model_name}\")\n",
    "    model = load_model(model_name)\n",
    "    if not model:\n",
    "        raise ValueError(f\"{model_name} is not trained yet!\")\n",
    "\n",
    "    model_expected_features = model.booster_.feature_name()\n",
    "    if set(model_expected_features) != set(feature_cols):\n",
    "        print(f\"New features are available! Might want to retrain model {model_name}.\")\n",
    "    print(f\"predicting tournament and validation for {model_name}\")\n",
    "    validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(validation_data.loc[:, model_expected_features])\n",
    "    tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(tournament_data.loc[:, model_expected_features])\n",
    "\n",
    "    # do different neutralizations\n",
    "    # neutralize our predictions to the riskiest features only\n",
    "    print(\"neutralizing to riskiest_50 for validation and tournament\")\n",
    "    validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=validation_data,\n",
    "                                                                            columns=[f\"preds_{model_name}\"],\n",
    "                                                                            neutralizers=riskiest_features,\n",
    "                                                                            proportion=0.8,\n",
    "                                                                            normalize=True,\n",
    "                                                                            era_col=ERA_COL)[f\"preds_{model_name}\"]\n",
    "    tournament_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=tournament_data,\n",
    "                                                                            columns=[f\"preds_{model_name}\"],\n",
    "                                                                            neutralizers=riskiest_features,\n",
    "                                                                            proportion=0.8,\n",
    "                                                                            normalize=True,\n",
    "                                                                            era_col=ERA_COL)[f\"preds_{model_name}\"]\n",
    "\n",
    "    pred_cols.add(f\"preds_{model_name}\")\n",
    "    pred_cols.add(f\"preds_{model_name}_neutral_riskiest_50\")\n",
    "\n",
    "\n",
    "# rank per era for each prediction column so that we can combine safely\n",
    "validation_data[list(pred_cols)] = validation_data.groupby(ERA_COL).apply(lambda d: d[list(pred_cols)].rank(pct=True))\n",
    "tournament_data[list(pred_cols)] = tournament_data.groupby(ERA_COL).apply(lambda d: d[list(pred_cols)].rank(pct=True))\n",
    "# make ensembles for val and tournament\n",
    "print('creating ensembles for tournament and validation')\n",
    "validation_data[\"ensemble_neutral_riskiest_50\"] = sum(\n",
    "    [validation_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n",
    "    pct=True)\n",
    "tournament_data[\"ensemble_neutral_riskiest_50\"] = sum(\n",
    "    [tournament_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n",
    "    pct=True)\n",
    "ensemble_cols.add(\"ensemble_neutral_riskiest_50\")\n",
    "\n",
    "validation_data[\"ensemble_not_neutral\"] = sum(\n",
    "    [validation_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n",
    "tournament_data[\"ensemble_not_neutral\"] = sum(\n",
    "    [tournament_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n",
    "ensemble_cols.add(\"ensemble_not_neutral\")\n",
    "\n",
    "validation_data[\"ensemble_all\"] = sum([validation_data[pred_col] for pred_col in pred_cols]).rank(pct=True)\n",
    "tournament_data[\"ensemble_all\"] = sum([tournament_data[pred_col] for pred_col in pred_cols]).rank(pct=True)\n",
    "\n",
    "ensemble_cols.add(\"ensemble_neutral_riskiest_50\")\n",
    "ensemble_cols.add(\"ensemble_not_neutral\")\n",
    "ensemble_cols.add(\"ensemble_all\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"getting final validation stats\")\n",
    "# get our final validation stats for our chosen model\n",
    "validation_stats = validation_metrics(validation_data, [best_pred_col], example_col=EXAMPLE_PREDS_COL,\n",
    "                                      fast_mode=False)\n",
    "print(validation_stats.to_markdown())\n",
    "\n",
    "# rename best model to prediction and rank from 0 to 1 to meet diagnostic/submission file requirements\n",
    "validation_data[\"prediction\"] = validation_data[best_pred_col].rank(pct=True)\n",
    "tournament_data[\"prediction\"] = tournament_data[best_pred_col].rank(pct=True)\n",
    "#validation_data[\"prediction\"].to_csv(PATH+ \"/prediction_files/validation_data_\"+\"_\"+str(len(feature_cols))+\"_round\"+str(current_round)+\".csv\", index=True)\n",
    "tournament_data[\"prediction\"].to_csv(PATH+ \"/prediction_files/tournament_pred_\"+\"n\"+str(len(feature_cols))+\\\n",
    "                                     \"_round\"+str(current_round)+\".csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff85d9",
   "metadata": {},
   "source": [
    "# numeracli をアップデートしたらErrorでないかもしれないがLegacyができなくなるかも。それまではマニュアルアップデート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cabd76a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 16:55:01,251 INFO numerapi.base_api: uploading predictions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded prediction to KMTK49\n",
      "\r"
     ]
    }
   ],
   "source": [
    "#submission_id = napi.upload_predictions(\n",
    "#\"./sub/predictions_xg_std_n11_seed_ave.csv\", model_id= model_1,version=2)\n",
    "submission_id = napi.upload_predictions(\n",
    "PATH+ \"/prediction_files/tournament_pred_\"+\"n\"+str(len(feature_cols))+\\\n",
    "                                     \"_round\"+str(current_round)+\".csv\", model_id= model_2,version=2)\n",
    "#submission_id = napi.upload_predictions(\n",
    "#\"./sub/predictions_xg_std_n11_seed_ave.csv\", model_id= model_3,version=2)\n",
    "print('uploaded prediction to KMTK49')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "053bd9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "tournament_data[\"prediction\"].to_csv(PATH+ \"/prediction_files/tournament_pred_\"+\"n\"+str(len(feature_cols))+\\\n",
    "                                     \"_round\"+str(current_round)+\".csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53c8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
